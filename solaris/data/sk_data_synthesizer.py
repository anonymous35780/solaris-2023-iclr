"""
Copyright 2022 Arash Mehrjou, GlaxoSmithKline plc, Claire Lyle, University of Oxford, Pascal Notin, University of Oxford
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""


import six
import os
import numpy as np
import sklearn.datasets as dt
from abc import ABCMeta, abstractmethod
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
from slingpy.utils.path_tools import PathTools
from solaris.benchmarks.abstract_data_synthesizer import AbastractDataSynthesizer
import pickle


class SKDataSynthesizer(AbastractDataSynthesizer):
    """
    Class to generate synthetic datasets for benchmarking Solaris methods
    """

    def __init__(
        self,
        mode: AnyStr = "regression",  # [classification|regression|friedman1|friedman2|friedman3]
        n_samples: int = 100,
        n_features: Optional[int] = 2,
        n_informative: Optional[int] = 2,
        n_targets: Optional[int] = 1,
        effective_rank: Optional[int] = None,
        tail_strength: Optional[float] = 0.5,
        n_classes: Optional[int] = 2,
        n_clusters_per_class: Optional[int] = 2,
        n_redundant: Optional[int] = 2,
        noise_std: Optional[float] = 0.0,
        seed: Optional[int] = 2022,
        saved_directory: Optional[AnyStr] = None,
        **kwargs,
    ):
        """
        Initialize the data generator class.


        Args:
            mode (str): [classification|regression|friedman1|friedman2|friedman3]. See: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/_samples_generator.py
            n_samples (int):
            n_features (int): The number of features.
            n_informative (int): The number of informative features, e.g., the number of features used to build the linear model used to generate the output.
            n_targets (int): The dimension of the y output vector associated with a sample (relevant for regression only).
            effective_rank (int): The approximate number of singular vectors required to explain most of the input data by linear combinations [regression only]. If None, input is standard normal
            tail_strength (float): The relative importance of the fat noisy tail of the singular values profile if effective_rank is not None [regression only]
            n_classes (int): The number of classes (or labels) of the classification problem. [classification only]
            n_clusters_per_class (int): The number of clusters per class. [classification only]
            n_redundant (int): The number of redundant features. These features are generated as random linear combinations of the informative features [classification only]
            noise_std (float): The standard deviation of the gaussian noise applied to the output.
            seed (int): The random seed. Keep it fixed to generate the same datasets.
        """

        super(SKDataSynthesizer, self).__init__(
            mode, n_samples, n_features, n_targets, noise_std, seed
        )
        self.n_informative = n_informative
        self.effective_rank = effective_rank
        self.n_classes = n_classes
        self.n_clusters_per_class = n_clusters_per_class
        self.n_redundant = n_redundant
        self.tail_strength = tail_strength
        self.saved_directory = saved_directory
        self.kwargs = kwargs

    def _generate_data(self):
        if self.mode == "regression":

            # The input set can either be well conditioned (std normal) or have a low rank-fat tail singular profile.
            # The output is generated by applying a (potentially biased) random linear regression model with n_informative
            # nonzero regressors to the previously generated input and some gaussian centered noise with some adjustable scale.
            self.dataset = dt.make_regression(
                n_samples=self.n_samples,
                n_features=self.n_features,
                n_informative=self.n_informative,
                n_targets=self.n_targets,
                effective_rank=self.effective_rank,
                tail_strength=self.tail_strength,
                noise=self.noise_std,
                random_state=self.seed,
                **self.kwargs,
            )
        elif self.mode == "classification":
            # This initially creates clusters of points normally distributed (std=1) about vertices of an n_informative-dimensional hypercube
            # with sides of length 2*class_sep and assigns an equal number of clusters to each class. It introduces interdependence between these
            # features and adds various types of further noise to the data.
            self.dataset = dt.make_classification(
                n_samples=self.n_samples,
                n_features=self.n_features,
                n_redundant=self.n_redundant,
                n_classes=self.n_classes,
                n_clusters_per_class=self.n_clusters_per_class,
                noise=self.noise_std,
                random_state=self.seed,
                **self.kwargs,
            )
        elif self.mode == "friedman1":
            assert (
                self.n_features >= 5
            ), "Number of features has to be at least 5 for Friedman #1 regression problem"
            self.n_features = self.n_features
            self.dataset = dt.make_friedman1(
                n_samples=self.n_samples,
                n_features=self.n_features,
                noise=self.noise_std,
                random_state=self.seed,
            )
        elif self.mode == "friedman2":
            self.n_features = 4
            self.dataset = dt.make_friedman2(
                n_samples=self.n_samples, noise=self.noise_std, random_state=self.seed
            )
        elif self.mode == "friedman3":
            self.n_features = 4
            self.dataset = dt.make_friedman3(
                n_samples=self.n_samples, noise=self.noise_std, random_state=self.seed
            )

        if self.saved_directory:
            datadict = dict()
            datadict["metadata"] = self.get_dataset_metadata()
            datadict["data"] = self.dataset
            filepath = os.path.join(self.saved_directory, "datadict.pkl")
            PathTools.mkdir_if_not_exists(self.saved_directory)
            with open(filepath, "wb") as fp:
                pickle.dump(datadict, filepath, pickle.HIGHEST_PROTOCOL)

    def get_groundtruth_function(self):
        pass


if __name__ == "__main__":
    # Example syntehtic dataset generation and ploting
    params = {
        "mode": "regression",
        "n_samples": 100,
        "n_features": 2,
        "n_targets": 1,
        "effective_rank": 2,
        "noise": 1.0,
        "seed": 2022,
        # "coef": True
    }
    synth = DataSynthesizer(**params)
    X, y, coef = synth.generate_dataset()
